
data.dir = ./data
wikipedia.json.dump= FIXME
tmp.dir = /tmp

jdbm.dir = jdbm
jdbm.commit = 1000

hadoop.title2id = ./data/title2id
hadoop.id2title = ./data/id2title

lucene.index = ./data/lucene
hadoop.spots = ./data/spot-index
hadoop.id2incoming = ./data/id2incoming

 
spot.commonness.threshold = 0.005
spot.probability.threshold = 0.02



#number of entities in the collection
w = 4700000

#relatedness function
relatedness=milne

#how many words consider around the spot
context.window.size = 50

incoming.nodes = ./data/graph/incoming-edges.tsv.gz
outcoming.nodes = ./data/graph/outcoming-edges.tsv.gz

ram.incoming.nodes = ./data/graph/incoming-edges.bin
ram.outcoming.nodes = ./data/graph/outcoming-edges.bin

# use the context text to rank the entities (affects performance)
rank.by.similarity = false
# use the prior probability (e|s) aka commonness to rank the entities 
rank.by.prior = true

lucene.wiki.id =./data/lucene/wiki2lucene-map.bin

# type of the spot repository [mapfile|mapdb]
spot.repository=ram
relatedness.cache.size=5000
spotter.cache.size=10000

prior.threshold=0.03

# where to store the minimal perfect hash function for spots
ram.spot.hash.values = ./data/spot/ram/spot-hashes.gz

ram.spot.perfect.hash = ./data/spot/ram/minimal-perfect-hash.bin
ram.spot.offsets = ./data/spot/ram/offsets.gz
ram.spot.offsets.ef = ./data/spot/ram/offsets.eliasfano.bin
ram.spot.data.bin = ./data/spot/ram/spots.bin

wikiminer.classifier = ./data/wikiminer/model.arff

spots = ./data/spot/spots.tsv.gz




